{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e799fd31-4a08-408e-baa1-77f9aaae48a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%maven gov.nist.math:jama:1.0.2\n",
    "import Jama.*;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87ae9de4-0417-4a51-9b4f-30050a0c904e",
   "metadata": {},
   "outputs": [],
   "source": [
    "/**\n",
    " * Bush School CPJava Class Final Project\n",
    " * Project Details: https://chandrunarayan.github.io/cpjava/final_projects/\n",
    " * static MatrixUtil class with static mprint() dnd mcrud() functions\n",
    " * used for creating matrices of random number weights and printing etc.\n",
    " */\n",
    "static class MatrixUtil {\n",
    "\n",
    "  /**\n",
    "   * mprint function for debugging.\n",
    "   * @param title: description of print outs following\n",
    "   * @param in: input Matrix to print\n",
    "   */\n",
    "  static void mprint(boolean debug, String title, Matrix in) {\n",
    "    if (debug) {\n",
    "      System.out.println(String.format(\"%s:\", title));\n",
    "      // caling the Matrix print function\n",
    "      in.print(3, 3);  // print 3 digits of precision for rows and cols\n",
    "    }\n",
    "  }\n",
    "\n",
    "  /**\n",
    "   * mcrud function for creating a random uniform distribution weights matrix.\n",
    "   * @param rows: number of rows\n",
    "   * @param cols: number of cols\n",
    "   * @return out: return created matrix\n",
    "   */\n",
    "  static Matrix mcrud(int rows, int cols) {\n",
    "    // caling the Matrix random() to create a matrix of weights\n",
    "    Matrix out = Matrix.random(rows, cols);\n",
    "    return out;\n",
    "  }\n",
    "  /**\n",
    "   * mmin function for finding min value in a matrix of values.\n",
    "   * @param m: Matrix in which to find min\n",
    "   * @return out: return max value\n",
    "   */\n",
    "  static double mmin(Matrix m) {\n",
    "    double[][] a = m.getArray();\n",
    "    double min = a[0][0];\n",
    "    int rows = a.length;\n",
    "    int cols = a[0].length;\n",
    "    for (int r = 0; r<rows; r++) {\n",
    "      for (int c = 0; c<cols; c++) {\n",
    "        min = a[r][c]<min ? a[r][c] : min;\n",
    "      }\n",
    "    }\n",
    "    return min;\n",
    "  }\n",
    "  /**\n",
    "   * mmax function for finding min value in a matrix of values.\n",
    "   * @param m: Matrix in which to find max\n",
    "   * @return out: return max value\n",
    "   */\n",
    "  static double mmax(Matrix m) {\n",
    "    double[][] a = m.getArray();\n",
    "    double max = a[0][0];\n",
    "    int rows = a.length;\n",
    "    int cols = a[0].length;\n",
    "    for (int r = 0; r<rows; r++) {\n",
    "      for (int c = 0; c<cols; c++) {\n",
    "        max = a[r][c]>max ? a[r][c] : max;\n",
    "      }\n",
    "    }\n",
    "    return max;\n",
    "  }\n",
    "  /**\n",
    "   * m2colp function for printing 2 single-col matrices side-by-side\n",
    "   * @param p: first column matrix\n",
    "   * @param q: second column matrix\n",
    "   */\n",
    "  static void m2colp(Matrix p, Matrix q) {\n",
    "    double[][] pA = p.getArray();\n",
    "    double[][] qA = q.getArray();\n",
    "    int pR = pA.length;\n",
    "    int pC = pA[0].length;\n",
    "    int qR = qA.length;\n",
    "    int qC = qA[0].length;\n",
    "    if (pR == qR && pC == 1 && qC == 1) { // 2 single col matrices of equal rows\n",
    "      for (int i = 0; i<pR; i++) {\n",
    "        System.out.println(String.format(\"%10.5f %10.5f\", pA[i][0], qA[i][0]));\n",
    "      }\n",
    "    } else {\n",
    "      System.out.println(\"input matrices are not 2 single col matrices of equal rows\");\n",
    "    }\n",
    "  }\n",
    "  /**\n",
    "   * m3colp function for printing 3 single-col matrices side-by-side\n",
    "   * @param p: first column matrix\n",
    "   * @param q: second column matrix\n",
    "   */\n",
    "  static void m3colp(Matrix p, Matrix q, Matrix r) {\n",
    "    double[][] pA = p.getArray();\n",
    "    double[][] qA = q.getArray();\n",
    "    double[][] rA = r.getArray();\n",
    "    int pR = pA.length;\n",
    "    int pC = pA[0].length;\n",
    "    int qR = qA.length;\n",
    "    int qC = qA[0].length;\n",
    "    int rR = rA.length;\n",
    "    int rC = rA[0].length;\n",
    "    if (pR == qR && qR == rR && pC == 1 && qC == 1 && rC == 1) { // 3 single col matrices of equal rows\n",
    "      for (int i = 0; i<pR; i++) {\n",
    "        System.out.println(String.format(\"%10.5f %10.5f %10.5f\", pA[i][0], qA[i][0], rA[i][0]));\n",
    "      }\n",
    "    } else {\n",
    "      System.out.println(\"input matrices are not 3 single col matrices of equal rows\");\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed4377ba-7d60-42bd-b230-ede3962e646b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "/**\n",
    " * Bush School CPJava Class Final Project\n",
    " * Project Details: https://chandrunarayan.github.io/cpjava/final_projects/\n",
    " * static Activator class with static sigmoid function \n",
    " */\n",
    "static class Activator {\n",
    "  /**\n",
    "   * sigmoid function for activation.\n",
    "   * @param in: input Matrix of weighted sums  \n",
    "   * @return out: calculated output matrix of sigmoid of weighted sums\n",
    "   */  \n",
    "  static Matrix sigmoid(Matrix in) {\n",
    "    \n",
    "    // first get the 2D array inside matrix\n",
    "    double [][] inA = in.getArray();\n",
    "    //  clone it\n",
    "    double [][] outA = in.getArrayCopy();\n",
    "    // put each weighted sum through the sigmoid function\n",
    "    for (int i = 0; i < inA.length; i++) {\n",
    "      for (int j = 0; j < inA[i].length; j++) {\n",
    "        outA[i][j] = 1.0/(1+Math.exp(-inA[i][j]));\n",
    "      }\n",
    "    }\n",
    "    \n",
    "    Matrix out = new Matrix(outA);\n",
    "    \n",
    "    return out;\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da238761-b402-44d5-8106-d68ccab4ed1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "void print_progress(int iter, Matrix out) {\n",
    "  Matrix output_error = target.minus(out);\n",
    "  String myStr = String.format(\"\\nprinting final_output, target and output_error from neural network after %d iterations\", iter);\n",
    "  System.out.println(myStr);\n",
    "  MatrixUtil.m3colp(out, target, output_error);\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32229473-544e-4efd-be55-248f75b7aea8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "/**\n",
    " * Bush School CPJava Class Final Project\n",
    " * Project Details: https://chandrunarayan.github.io/cpjava/final_projects/\n",
    " * NeuralNetwork class with predict() and train() functions\n",
    " */\n",
    "class NeuralNetwork {\n",
    "  /** number of input nodes */\n",
    "  int iNodes;\n",
    "  /** number of hidden nodes */\n",
    "  int hNodes;\n",
    "  /** number of output nodes */\n",
    "  int oNodes;\n",
    "  /** learning rate */\n",
    "  double lRate;\n",
    "  /** weights matrix in between input and hidden layers */\n",
    "  Matrix wIH;\n",
    "  /** weights matrix in between hidden and output layers */\n",
    "  Matrix wHO;\n",
    "\n",
    "  /**\n",
    "   * Constructor for the Neural Network Class.\n",
    "   * Initialize properties\n",
    "   */\n",
    "  /** initialize nodes and lr */\n",
    "  NeuralNetwork(int iNodes_, int hNodes_, int oNodes_, double lRate_) {\n",
    "    iNodes = iNodes_;\n",
    "    hNodes = hNodes_;\n",
    "    oNodes = oNodes_;\n",
    "    lRate = lRate_;\n",
    "    \n",
    "    boolean debug = false;\n",
    "\n",
    "    // setup weights\n",
    "\n",
    "    // initial input_hidden weights\n",
    "    // created using a normal distribution of random numbers\n",
    "    //wIH = MatrixUtil.mcrud(iNodes, hNodes);  // weights matrix in between input and hidden layers\n",
    "    //MatrixUtil.mprint(debug, \"printing initial input_hidden weights\", wIH);\n",
    "\n",
    "    // initial hidden_output weights\n",
    "    // created using a normal distribution of random numbers\n",
    "    //wHO = MatrixUtil.mcrud(hNodes, oNodes);  // weights matrix in between hidden and output layers\n",
    "    //MatrixUtil.mprint(debug, \"printing initial hidden_output weights\", wHO);\n",
    "    \n",
    "    double[][] awIH = {{0.9, 0.3, 0.4},\n",
    "      {0.2, 0.8, 0.2},\n",
    "      {0.1, 0.5, 0.6}};\n",
    "    wIH = new Matrix(awIH);  // weights matrix in between input and hidden layers\n",
    "    MatrixUtil.mprint(debug, \"printing initial input_hidden weights\", wIH);\n",
    "  \n",
    "    // initial hidden_output weights\n",
    "    // this will eventually be created using a normal distribution of random numbers\n",
    "    double[][] awHO = {{0.3, 0.7, 0.5},\n",
    "      {0.6, 0.5, 0.2},\n",
    "      {0.8, 0.1, 0.9}};\n",
    "    wHO = new Matrix(awHO);  // weights matrix in between hidden and output layers\n",
    "    MatrixUtil.mprint(debug, \"printing initial hidden_output weights\", wHO);\n",
    "    \n",
    "}\n",
    "\n",
    "  /**\n",
    "   * predict() function implementing feed forward calculations.\n",
    "   * @param inp: Matrix of input values to Neural Network\n",
    "   * @return res: Matrix [] an array of matrices with calculated output values from the Neural Network\n",
    "   */\n",
    "  Matrix [] predict(Matrix inp_) {\n",
    "    boolean debug = false;\n",
    "    // create Matrix array to store hidden_input, hidden_output, and final_output values\n",
    "    Matrix [] res = new Matrix [3];\n",
    "\n",
    "    // hidden layer calculations\n",
    "    // hidden layer inputs: weighted sum\n",
    "    Matrix hid_inp = wIH.times(inp_);  // dot product to create the weighted sum\n",
    "    MatrixUtil.mprint(debug, \"printing hidden layer inputs: weighted sum\", hid_inp);\n",
    "    res[0] = hid_inp;  // store hidden weighted sum in in res in Matrix array\n",
    "    \n",
    "    // hidden layer outputs: sigmoid(weighted sum)\n",
    "    // note: output of hidden layer is same as input of output layer\n",
    "    Matrix hid_outp = Activator.sigmoid(hid_inp);  // sigmoid activation of the weighted sum\n",
    "    MatrixUtil.mprint(debug, \"printing hidden layer outputs: sigmoid(weighted sum)\", hid_outp);\n",
    "    res[1] = hid_outp; // store hidden sigmoid output in res Matrix array\n",
    "    \n",
    "    //output layer inputs: weighted sum\n",
    "    //input to output layer is same as output from hidden layer\n",
    "    Matrix out_inp = wHO.times(hid_outp);  // dot product to create the weighted sum\n",
    "    MatrixUtil.mprint(debug, \"printing output layer inputs: weighted sum\", out_inp);\n",
    "    \n",
    "    // calculate sigmoid activation of the weighted sum of the output layer\n",
    "    Matrix out_outp = Activator.sigmoid(out_inp);\n",
    "    MatrixUtil.mprint(debug, \"printing output layer outputs : sigmoid(weighted sum)\", out_outp);\n",
    "    res[2] = out_outp;  // store hidden sigmoid output in res Matrix array\n",
    "    \n",
    "    return res;  // return the sigmoid activation of the weighted sum\n",
    "  }\n",
    "\n",
    "  /**\n",
    "   * train() function for implementing backward propagation.\n",
    "   * @param inp: Matrix of input values to train the Neural Network\n",
    "   */\n",
    "  void train(Matrix inp_, Matrix tgt_) {\n",
    "    // Back Propagation\n",
    "    // first feed forward!\n",
    "    \n",
    "    boolean debug = false;\n",
    "    \n",
    "    //System.out.println(\"***** Feed Forward Starts *******\");\n",
    "    \n",
    "    MatrixUtil.mprint(debug, \"printing inputs to neural network\", inp_);\n",
    "    MatrixUtil.mprint(debug, \"printing targets to neural network\", tgt_);\n",
    "    \n",
    "    Matrix [] res = this.predict(inp_);\n",
    "    \n",
    "    MatrixUtil.mprint(debug, \"printing hidden layer inputs: weighted sum\", res[0]);\n",
    "    MatrixUtil.mprint(debug, \"printing hidden layer outputs: sigmoid(weighted sum)\", res[1]);\n",
    "    MatrixUtil.mprint(debug, \"printing output layer inputs = hidden_layer outputs\", res[1]);\n",
    "    MatrixUtil.mprint(debug, \"printing output layer outputs = final outputs\", res[2]);\n",
    "    \n",
    "    Matrix output_error = tgt_.minus(res[2]);\n",
    "    MatrixUtil.mprint(debug, \"printing output_error from neural network\", output_error);\n",
    "\n",
    "    Matrix hidden_error = wHO.transpose().times(output_error);\n",
    "    MatrixUtil.mprint(debug, \"printing hidden errors from neural network\", hidden_error);    \n",
    "    \n",
    "    //System.out.println(\"***** Back Propagations Starts *******\");\n",
    "    \n",
    "    Matrix unity = new Matrix(hidden_nodes,1,1.0);  // Create a column matrix of 1.0 to use in 1-sigmoid calculation\n",
    "    \n",
    "    // Update weights between hidden and output layers based on output error\n",
    "    Matrix lhdot1 = output_error.arrayTimes(res[2].arrayTimes(unity.minus(res[2])));\n",
    "    Matrix lhdot2 = res[1].transpose();\n",
    "    wHO.plusEquals(lhdot1.times(lhdot2).times(lRate));\n",
    "    MatrixUtil.mprint(debug, \"printing updated weights between hidden and output layers\", wHO);\n",
    "    \n",
    "    // Update weights between input and hidden layers based on calculated error\n",
    "    Matrix lhdot3 = hidden_error.arrayTimes(res[1].arrayTimes(unity.minus(res[1])));\n",
    "    Matrix lhdot4 = inp_.transpose();\n",
    "    wIH.plusEquals(lhdot3.times(lhdot4).times(lRate));\n",
    "    MatrixUtil.mprint(debug, \"printing updated weights between input and hidden layers\", wIH);\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6e2baad-adaf-4191-b793-01a3ddd46942",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "/**\n",
    " * Bush School CPJava Class Final Project\n",
    " * Project Details: https://chandrunarayan.github.io/cpjava/final_projects/\n",
    " * 1. Build a complete Java Neural Network from scratch\n",
    " * 2. Test the Neural Network using 2 scenarios\n",
    " *    a. Predict equation of line using a supplied set of points\n",
    " *    b. Classify hand written 28x28 pixel numerals from 0-9\n",
    " * Adapted for Bush School by Chandru Narayan\n",
    " * from \"Make your own Neural Network\" by Tariq Rashid\n",
    " */\n",
    "\n",
    "// Import NIST Java Matrix Library\n",
    "// https://math.nist.gov/javanumerics/jama/Jama-1.0.3.jarhttps://math.nist.gov/javanumerics/jama/Jama-1.0.3.jar\n",
    "// https://math.nist.gov/javanumerics/jama/doc/\n",
    "\n",
    "// Globals\n",
    "NeuralNetwork bushNN; // the neural network\n",
    "Matrix input, target, output[]; // input, target, and output Matrix globals\n",
    "int input_nodes = 3;\n",
    "int hidden_nodes = 3;\n",
    "int output_nodes = 3;\n",
    "double learning_rate = 0.3;\n",
    "int nIter = 100;  // number of iterations corresponding to number of input data for training\n",
    "int pIter = 10;   // print final output only after every pIter iterations\n",
    "\n",
    "boolean debug = false;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c80946f3-4015-4d30-82b8-aac8cca1bdd8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bushNN = new NeuralNetwork(input_nodes, hidden_nodes, output_nodes, learning_rate);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63ff258e-6d57-427a-aa43-99fe216b2473",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing initial input_hidden weights:\n",
      "\n",
      " 0.900 0.300 0.400\n",
      " 0.200 0.800 0.200\n",
      " 0.100 0.500 0.600\n",
      "\n",
      "printing initial hidden_output weights:\n",
      "\n",
      " 0.300 0.700 0.500\n",
      " 0.600 0.500 0.200\n",
      " 0.800 0.100 0.900\n",
      "\n"
     ]
    }
   ],
   "source": [
    "MatrixUtil.mprint(true, \"printing initial input_hidden weights\", bushNN.wIH);\n",
    "MatrixUtil.mprint(true, \"printing initial hidden_output weights\", bushNN.wHO);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a56c7036-c495-4e71-a56f-e07fa3d0ed7d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing initial input to neural network:\n",
      "\n",
      " 0.947\n",
      " 0.124\n",
      " 0.829\n",
      "\n",
      "printing targets for neural network:\n",
      "\n",
      " 0.100\n",
      " 0.800\n",
      " 0.500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Create input data as a Column Matrix\n",
    "// carefully note how the column matrix is initialized\n",
    "// This will eventualy come from a file of input values\n",
    "double[][] ainp = {{0.9},\n",
    "{0.1},\n",
    "{0.8}};\n",
    "\n",
    "//input = new Matrix(ainp);  //column matrix of specific input values\n",
    "input = MatrixUtil.mcrud(input_nodes, 1);  // column matrix of random uniform input values\n",
    "MatrixUtil.mprint(true, \"printing initial input to neural network\", input);\n",
    "\n",
    "// Create target data as a Column Matrix\n",
    "// carefully note how the column matrix is initialized\n",
    "// This will eventualy come from a file of input values\n",
    "double[][] atgt = {{0.1},\n",
    "{0.8},\n",
    "{0.5}};\n",
    "\n",
    "target = new Matrix(atgt);  //column matrix of input values\n",
    "MatrixUtil.mprint(true, \"printing targets for neural network\", target);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "639842c0-61e2-4475-bc94-019eff072be7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "printing final_output, target and output_error from neural network after 10 iterations\n",
      "   0.61100    0.10000   -0.51100\n",
      "   0.70257    0.80000    0.09743\n",
      "   0.71661    0.50000   -0.21661\n",
      "\n",
      "printing final_output, target and output_error from neural network after 20 iterations\n",
      "   0.51231    0.10000   -0.41231\n",
      "   0.70267    0.80000    0.09733\n",
      "   0.67617    0.50000   -0.17617\n",
      "\n",
      "printing final_output, target and output_error from neural network after 30 iterations\n",
      "   0.43236    0.10000   -0.33236\n",
      "   0.74908    0.80000    0.05092\n",
      "   0.67382    0.50000   -0.17382\n",
      "\n",
      "printing final_output, target and output_error from neural network after 40 iterations\n",
      "   0.38410    0.10000   -0.28410\n",
      "   0.72599    0.80000    0.07401\n",
      "   0.63080    0.50000   -0.13080\n",
      "\n",
      "printing final_output, target and output_error from neural network after 50 iterations\n",
      "   0.32944    0.10000   -0.22944\n",
      "   0.75020    0.80000    0.04980\n",
      "   0.60943    0.50000   -0.10943\n",
      "\n",
      "printing final_output, target and output_error from neural network after 60 iterations\n",
      "   0.30631    0.10000   -0.20631\n",
      "   0.74473    0.80000    0.05527\n",
      "   0.58795    0.50000   -0.08795\n",
      "\n",
      "printing final_output, target and output_error from neural network after 70 iterations\n",
      "   0.26656    0.10000   -0.16656\n",
      "   0.76555    0.80000    0.03445\n",
      "   0.56854    0.50000   -0.06854\n",
      "\n",
      "printing final_output, target and output_error from neural network after 80 iterations\n",
      "   0.24410    0.10000   -0.14410\n",
      "   0.76937    0.80000    0.03063\n",
      "   0.56176    0.50000   -0.06176\n",
      "\n",
      "printing final_output, target and output_error from neural network after 90 iterations\n",
      "   0.21188    0.10000   -0.11188\n",
      "   0.78880    0.80000    0.01120\n",
      "   0.55091    0.50000   -0.05091\n",
      "\n",
      "printing final_output, target and output_error from neural network after 100 iterations\n",
      "   0.19871    0.10000   -0.09871\n",
      "   0.79229    0.80000    0.00771\n",
      "   0.53475    0.50000   -0.03475\n",
      "\n",
      "printing final adjusted input_hidden weights:\n",
      "\n",
      " 1.072 0.496 0.598\n",
      " 0.221 0.855 0.260\n",
      " -0.154 0.314 0.413\n",
      "\n",
      "\n",
      "printing final adjusted hidden_output weights:\n",
      "\n",
      " -0.981 -0.438 -0.557\n",
      " 0.814 0.691 0.378\n",
      " 0.228 -0.406 0.434\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for (int i = 0; i < nIter; i++) {\n",
    "    input = MatrixUtil.mcrud(input_nodes, 1);  // column matrix of random uniform input values for each iteration\n",
    "    // Train the neural network for a given set of training inputs for which answer is known!\n",
    "    // This is accomplished through backward propagation\n",
    "    bushNN.train(input, target);\n",
    "    if ((i+1)%pIter == 0) {\n",
    "      output = bushNN.predict(input);\n",
    "      String myStr = String.format(\"printing iteration %d output from neural network after adjusting weights\", i+1);\n",
    "      MatrixUtil.mprint(debug, myStr, output[2]);\n",
    "      print_progress(i+1, output[2]);\n",
    "    }\n",
    "}\n",
    "\n",
    "MatrixUtil.mprint(true, \"\\nprinting final adjusted input_hidden weights\", bushNN.wIH);\n",
    "MatrixUtil.mprint(true, \"\\nprinting final adjusted hidden_output weights\", bushNN.wHO);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f902d2e-45cc-400f-938d-e021244b0958",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Java",
   "language": "java",
   "name": "java"
  },
  "language_info": {
   "codemirror_mode": "java",
   "file_extension": ".jshell",
   "mimetype": "text/x-java-source",
   "name": "Java",
   "pygments_lexer": "java",
   "version": "11.0.18+10-post-Debian-1deb11u1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
